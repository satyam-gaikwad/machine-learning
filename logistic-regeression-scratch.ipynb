{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb40d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import log,dot,exp,shape\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1f693de",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd,yd = make_classification(n_features=4)\n",
    "x_tr,x_te,y_tr,y_te = train_test_split(xd,yd,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44bf654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    for i in range(shape(data)[1]):\n",
    "        data[:,i] = (data[:,i]-np.mean(data[:,i]))/np.std(data[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263517d",
   "metadata": {},
   "source": [
    "The data sets are always multidimensional. We will need to use matrices for any kind of calculation. So, for input, we have two matrices to deal with. The first one is for feature vectors, and the second is for parameters or weights. Our first matrix is of the mxn dimension, where m is the number of observations while n is the dimension of observations. And the second one is of nx1 dimension. Here, we will add a bias column of ones to our feature vectors matrix and a corresponding parameter term to the weight vector. Bias is important to make the model more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48703e74",
   "metadata": {},
   "source": [
    "Linear regression employs the least squared error as the cost function. But the least squared error function for logistic regression is non-convex. While performing gradient descent chances that we get stuck in a local minimum is more. So instead, we use log loss as the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31b5bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        sig = 1/(1+exp(-z))\n",
    "        return sig\n",
    "\n",
    "\n",
    "    def initialize(self,x):\n",
    "        weights = np.zeros((shape(x)[1]+1,1))\n",
    "        x = np.c_[np.ones((shape(x)[0],1)),x] #(100,5)\n",
    "        return weights,x\n",
    "\n",
    "\n",
    "    def standardize(data):\n",
    "        for i in range(shape(data)[1]):\n",
    "            data[:,i] = (data[:,i]-np.mean(data[:,i]))/np.std(data[:,i])\n",
    "    \n",
    "    def cost(self,x,y,theta):\n",
    "        #print(\"shape cost\" ,shape(x))\n",
    "        z = dot(x,theta)\n",
    "        cost0 = y.T.dot(log(self.sigmoid(z)))\n",
    "        cost1 = (1-y.T).dot(log(1-self.sigmoid(z)))\n",
    "        cost = -(cost0+cost1)/len(y)\n",
    "        return cost\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #gradient descent\n",
    "    def fit(self,x,y,alpha=0.001,iter=100):\n",
    "        weights,x = self.initialize(x)\n",
    "        cost_list = np.zeros(iter,)\n",
    "        for i in range(iter):\n",
    "            weights = weights - alpha * dot(x.T, self.sigmoid(dot(x,weights)) - np.reshape(y,(len(y),1)))\n",
    "            cost_list[i] = self.cost(x,y,weights)\n",
    "        self.weights = weights\n",
    "        return cost_list\n",
    "    \n",
    "\n",
    "    \n",
    "    # prediction\n",
    "    def predict(self,x):\n",
    "        z = dot(self.initialize(x)[1],self.weights)\n",
    "        lis = []\n",
    "        for i in self.sigmoid(z):\n",
    "            if i>0.5:\n",
    "                lis.append(1)\n",
    "            else:\n",
    "                lis.append(0)\n",
    "        return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26cd7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # F1 scores\n",
    "def f1_score(y,y_hat):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1 and y_hat[i] == 1:\n",
    "            tp += 1\n",
    "        elif y[i] == 1 and y_hat[i] == 0:\n",
    "            fn += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 1:\n",
    "            fp += 1\n",
    "        elif y[i] == 0 and y_hat[i] == 0:\n",
    "            tn += 1\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23c70a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9275362318840579\n",
      "0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "standardize(x_tr)\n",
    "standardize(x_te)\n",
    "test = LogisticRegression()\n",
    "model= test.fit(x_tr,y_tr)\n",
    "y_pred = test.predict(x_te)\n",
    "y_train = test.predict(x_tr)\n",
    "#Let's see the f1-score for training and testing data\n",
    "f1_score_tr = f1_score(y_tr,y_train)\n",
    "f1_score_te = f1_score(y_te,y_pred)\n",
    "print(f1_score_tr)\n",
    "print(f1_score_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95746f5b",
   "metadata": {},
   "source": [
    "### Now, let’s see how our logistic regression fares in comparison to sklearn’s logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "897d0c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "model = LogisticRegression().fit(x_tr,y_tr)\n",
    "y_pred = model.predict(x_te)\n",
    "print(f1_score(y_te,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e489191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d407aa69",
   "metadata": {},
   "source": [
    "#References\n",
    "https://www.analyticsvidhya.com/blog/2022/02/implementing-logistic-regression-from-scratch-using-python/\n",
    "https://www.kaggle.com/code/jagannathrk/logistic-regression-from-scratch-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
